{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6368998-f69e-474f-a952-44dd55b0a9e3",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d38cb5-417a-4df4-83c2-33cf500df2c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install sentence-transformers hf_xet\n",
    "#jina-embeddings benefit from FlashAttention-2\n",
    "#!pip install flash-attn --no-build-isolation\n",
    "#!pip install -v -U flash-attn\n",
    "!pip install ninja pyarrow\n",
    "!pip install pyod catboost\n",
    "!pip install ftfy emoji einops\n",
    "!pip install jupyter_capture_output\n",
    "!pip install --no-deps dask-expr\n",
    "!pip install scipy betacal\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d17e5-b63e-42f4-9851-7c63b461e8fc",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580fd342-e2d4-4756-8d66-d038351755a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time, os, re, torch, ftfy, emoji, joblib, jupyter_capture_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from tqdm.dask import TqdmCallback\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from pyod.models.hbos import HBOS\n",
    "from catboost import CatBoostClassifier\n",
    "from betacal import BetaCalibration\n",
    "from sklearn.metrics import classification_report, roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, roc_auc_score, average_precision_score\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"`torch_dtype` is deprecated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3fe50f-3635-4120-b8ce-28b5565469fd",
   "metadata": {},
   "source": [
    "## Experiment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde40900-9585-4120-ae40-04790f93033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# research code for HIPSTer project: Hybrid, Information, Psychological, Societal Threats\n",
    "# handling system for public security domain practitioners, businesses, and education (HIPSTer)\n",
    "\n",
    "dataset = \"LtHate_v1\" # choose text dataset: LtHate LtHate_v1 RuToxic DynaHate EnBerkeley EnToxiGen EnSuperset\n",
    "datasetFolder = './data/'\n",
    "resultsFolder = './results/'\n",
    "vectors = [\"potion\", \"snow\", \"jina\", \"e5\"] # choose among modern vectorizers: potion snow jina e5 gte\n",
    "chunk_size_setting = 1024 # large=8192 (if many texts or non-gte vectorizers + GPU) or small=512\n",
    "batch_size_setting = 32 # large=1024 (if many texts or non-gte vectorizers + GPU) or small=64\n",
    "compDevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "useDimensionalityReduction = False\n",
    "useCatBoostOutputCallibration = False\n",
    "\n",
    "k = 5 # Number of folds for StratifiedKFold cross-validation, i.e. 10-fold CV\n",
    "num_vars = 32 # Number of variables after dimensionality reduction with PCA\n",
    "num_tree = 500 # Maximum possible number of trees to grow for CatBoost model\n",
    "eval_kpi = \"AUC\" # Success metric to track performance on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b0664-ebba-4241-9940-0db66c87aef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvLogFilename = resultsFolder + dataset + \"-CV-log.txt\"\n",
    "resultsTableFile = resultsFolder + dataset + \"-table.txt\"\n",
    "resultsTableSummary = resultsFolder + dataset + \"-table.csv\"\n",
    "rocPlotFilename = resultsFolder + dataset + \"-fig-ROC.png\"\n",
    "prcPlotFilename = resultsFolder + dataset + \"-fig-PRC.png\"\n",
    "if not os.path.exists(resultsFolder):\n",
    "    os.makedirs(resultsFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b577d-82b7-4512-b29b-1ab7c2b5204a",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9219217-9e49-42a5-bf0a-546df9cfb279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_punctuation(text, toneDown=True):\n",
    "    # First, use ftfy to fix any encoding issues\n",
    "    if hasattr(text, '__len__'):\n",
    "        text = ftfy.fix_text(text)\n",
    "\n",
    "        # Custom rules for punctuation fixing\n",
    "        rules = [\n",
    "            # Remove http and https links\n",
    "            (r'https?://\\S+', ''),\n",
    "            # Remove consecutive repetitive punctuation, but keep a maximum of two for emphasis (e.g., !!)\n",
    "            (r'([,\\.?!])\\1{2,}', r'\\1\\1'),\n",
    "            # Add space after comma, period, question mark, or exclamation mark if not followed by space\n",
    "            (r'([,\\.?!])(?=[^\\s])', r'\\1 '),\n",
    "            # Remove space before comma, period, question mark, or exclamation mark\n",
    "            (r'\\s+([,\\.?!])', r'\\1'),\n",
    "            # Fix multiple spaces\n",
    "            (r'\\s{2,}', ' '),\n",
    "            # Ensure numbers have space before and after, except when punctuation or hyphen follows\n",
    "            (r'(\\d)(?=[^\\s\\d,\\.?!-])', r'\\1 '),\n",
    "            (r'(?<=[^\\s\\d-])(\\d)', r' \\1')\n",
    "        ]\n",
    "\n",
    "        if toneDown:\n",
    "            rules.append((r'[?!]', '.'))\n",
    "\n",
    "        # Replace emoji with :shortcode:\n",
    "        text = emoji.demojize(text, delimiters=(\" ::\", \":: \"))\n",
    "\n",
    "        # Apply each rule\n",
    "        for pattern, replacement in rules:\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "\n",
    "        text = text.strip()\n",
    "    else:\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "def get_vectorizer_link(name: str) -> str:\n",
    "    \"\"\"Return full Hugging Face path for a given short vectorizer name.\"\"\"\n",
    "    if name == \"jina\":\n",
    "        return \"jinaai/jina-embeddings-v3\"\n",
    "    elif name == \"snow\":\n",
    "        return \"Snowflake/snowflake-arctic-embed-l-v2.0\"\n",
    "    elif name == \"gte\":\n",
    "        return \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"\n",
    "    elif name == \"potion\":\n",
    "        return \"minishlab/potion-base-2M\"\n",
    "    elif name == \"e5\":\n",
    "        return \"intfloat/multilingual-e5-large-instruct\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown vectorizer name: {name}\")\n",
    "\n",
    "def clean_and_vectorize(df, fix_punctuation, sentvec=\"e5\", device=\"cuda\", normalize_embeddings=False, chunk_size=256, batch_size=32):\n",
    "    \"\"\"\n",
    "    Cleans text data and vectorizes it using a sentence transformer model.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input pandas DataFrame containing text data.\n",
    "    - fix_punctuation (function): Function to preprocess and fix punctuation in text.\n",
    "    - sentvec (str): Identifier for sentence vectorization method, e.g., \"e5-large-instruct\".\n",
    "    - device (str, default=\"cuda\"): Device to use for the sentence transformer model, e.g., \"cpu\" or \"cuda\".\n",
    "    - normalize_embeddings (bool): Whether to normalize embeddings.\n",
    "    - chunk_size (int, default=1024): Number of text samples per chunk for encoding.\n",
    "    - batch_size (int, default=128): Batch size for encoding in the transformer.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the resulting embeddings with appropriate column names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the sentence transformer model based on `sentvec`\n",
    "    if sentvec == \"jina\":\n",
    "        st = SentenceTransformer(get_vectorizer_link(sentvec), trust_remote_code=True, device=device)\n",
    "    else:\n",
    "        st = SentenceTransformer(get_vectorizer_link(sentvec), device=device)\n",
    "\n",
    "    # Partition data for parallel processing\n",
    "    n_partitions = mp.cpu_count()\n",
    "    ddf = dd.from_pandas(df, npartitions=n_partitions)\n",
    "\n",
    "    with TqdmCallback(desc=\"Cleaning text\"):\n",
    "        texts = ddf.apply(lambda x: fix_punctuation(x.iloc[1]), axis=1, meta=pd.Series(dtype=\"str\")).compute(scheduler=\"processes\").tolist()\n",
    "\n",
    "    # Determine prompt for the selected sentence vectorization method\n",
    "    st_prompt = \"query: \" if sentvec == \"e5\" else \"\"\n",
    "\n",
    "    # Process texts in chunks for memory efficiency\n",
    "    results = []\n",
    "    total_chunks = (len(texts) + chunk_size - 1) // chunk_size  # Calculate total number of chunks\n",
    "    width = len(str(total_chunks))  # Width for chunk number formatting\n",
    "\n",
    "    for i in range(0, len(texts), chunk_size):\n",
    "        torch.cuda.empty_cache()  # Clear GPU cache if necessary\n",
    "        current_chunk = i // chunk_size + 1  # Current chunk number\n",
    "        print(f\"\\rChunk: {current_chunk:0{width}}/{total_chunks:0{width}} \", end='', flush=True)\n",
    "        chunk = texts[i:i + chunk_size]\n",
    "        result = st.encode(chunk, batch_size=batch_size, normalize_embeddings=normalize_embeddings, show_progress_bar=True)\n",
    "        results.append(result)\n",
    "\n",
    "    # Clean up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate results and format as DataFrame\n",
    "    X = np.concatenate(results, axis=0)\n",
    "    df_embeddings = pd.DataFrame(X)\n",
    "    df_embeddings.columns = [f'X{i+1}' for i in range(df_embeddings.shape[1])]\n",
    "\n",
    "    return df_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63365b1d-d6f7-4b28-a17d-fe617d9ae05d",
   "metadata": {},
   "source": [
    "## Load & vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e933d3de-8d9a-48f7-9a97-12c293e20560",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load text comments dataset\n",
    "if dataset == 'DynaHate':\n",
    "    df = pd.read_csv(datasetFolder + dataset + '.csv', engine='python', usecols=['text', 'label'])\n",
    "    df = df[df.columns[::-1]]\n",
    "    df.columns = [0, 1]\n",
    "    di = {\"nothate\": 0, \"hate\": 1}\n",
    "    y = df[0].map(di)\n",
    "elif dataset == 'EnBerkeley':\n",
    "    df = pd.read_csv(datasetFolder + dataset + '.csv', engine='python', usecols=['text', 'contains_hate'])\n",
    "    df = df[df.columns[::-1]]\n",
    "    df.columns = [0, 1]\n",
    "    y = 1 - df[0]\n",
    "elif dataset == 'EnSuperset':\n",
    "    df = pd.read_csv(datasetFolder + dataset + '.csv', engine='python', usecols=['text', 'labels'])\n",
    "    df = df[df.columns[::-1]]\n",
    "    df.columns = [0, 1]\n",
    "    y = df[0]\n",
    "elif dataset == 'EnToxiGen':\n",
    "    df = pd.read_csv(datasetFolder + dataset + '.csv', engine='python', usecols=['generation', 'prompt_label'])\n",
    "    df = df[df.columns[::-1]]\n",
    "    df.columns = [0, 1]\n",
    "    y = df[0]\n",
    "elif dataset == 'LtHate':\n",
    "    dfA = pd.read_csv(datasetFolder + dataset + '.csv', engine='python')\n",
    "    dfA = dfA[dfA.columns[::-1]]\n",
    "    dfA.columns = [0, 1]\n",
    "    di = {\"No\": 0, \"Yes\": 1}\n",
    "    yA = dfA[0].map(di)\n",
    "    dfB = pd.read_csv(datasetFolder + 'Semantika_2.txt', sep='__', header=None, usecols=[2], engine='python')\n",
    "    dfB = dfB[2].str.split(\" \", n=1, expand=True)\n",
    "    di = {\"neutral\": 0, \"offensive\": 1}\n",
    "    yB = dfB[0].map(di)\n",
    "    df = pd.concat([dfA, dfB], ignore_index=True)\n",
    "    y = pd.concat([yA, yB], ignore_index=True)\n",
    "elif dataset == 'LtHate_v1':\n",
    "    df = pd.read_csv(datasetFolder + dataset + '.csv', engine='python')\n",
    "    df = df[df.columns[::-1]]\n",
    "    df.columns = [0, 1]\n",
    "    di = {\"No\": 0, \"Yes\": 1}\n",
    "    y = df[0].map(di)\n",
    "elif dataset == 'LtEmocionalumas':\n",
    "    df = pd.read_csv(datasetFolder + dataset + '.csv', engine='python')\n",
    "    df = df[df.columns[::-1]]\n",
    "    df.columns = [0, 1]\n",
    "    di = {\"No\": 0, \"Low\": 0, \"Medium\": 1, \"High\": 1, \"Critical\": 1}\n",
    "    y = df[0].map(di)\n",
    "elif dataset == 'LtManipuliacijos':\n",
    "    excel_file = pd.ExcelFile(datasetFolder + dataset + '.xlsx')\n",
    "    all_comments = []\n",
    "    all_labels = []\n",
    "    for sheet_name in excel_file.sheet_names:\n",
    "        df = pd.read_excel(datasetFolder + 'LtManipuliacijos.xlsx', sheet_name=sheet_name, usecols=[1])\n",
    "        sheet_comments = df['Komentaras'].astype(str).str.strip()\n",
    "        sheet_comments = sheet_comments[sheet_comments != ''].tolist()\n",
    "        all_comments.extend(sheet_comments)\n",
    "        if sheet_name == 'Manipuliaciniai':\n",
    "            all_labels.extend([1] * len(sheet_comments))\n",
    "        else:\n",
    "            all_labels.extend([0] * len(sheet_comments))\n",
    "    df = pd.DataFrame({0: all_labels, 1: all_comments})\n",
    "    y = df[0]      \n",
    "elif dataset == 'RuToxic':\n",
    "    df = pd.read_csv(datasetFolder + dataset + '.csv', engine='python')\n",
    "    df = df[df.columns[::-1]]\n",
    "    df.columns = [0, 1]\n",
    "    y = df[0]\n",
    "else:\n",
    "    df = pd.read_csv('Semantika_2.txt', sep='__', header=None, usecols=[2], engine='python')\n",
    "    df = df[2].str.split(\" \", n=1, expand=True)\n",
    "    di = {\"neutral\": 0, \"offensive\": 1}\n",
    "    y = df[0].map(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729c95b-ead3-4041-acc3-a2e7a8923e74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Vectorize text data\n",
    "for sentvec in vectors:\n",
    "    xVarFilename = datasetFolder + dataset + \"-X-\" + sentvec + \".parquet\"\n",
    "    files_exist_condition = os.path.exists(xVarFilename)\n",
    "    print(xVarFilename)\n",
    "\n",
    "    if not files_exist_condition:\n",
    "        if compDevice == \"cpu\":\n",
    "            print(\"Vectorizing text on CPU...\")\n",
    "            df_post = clean_and_vectorize(df, fix_punctuation, sentvec, compDevice, False, chunk_size_setting, batch_size_setting)\n",
    "            df_post.to_parquet(xVarFilename, engine=\"pyarrow\")\n",
    "        else:\n",
    "            try:\n",
    "                print(\"Vectorizing text on GPU 0...\")\n",
    "                torch.cuda.set_device(0)\n",
    "                torch.cuda.empty_cache()\n",
    "                df_post = clean_and_vectorize(df, fix_punctuation, sentvec, compDevice, False, chunk_size_setting, batch_size_setting)\n",
    "                df_post.to_parquet(xVarFilename, engine=\"pyarrow\")\n",
    "            except RuntimeError as e:\n",
    "                if \"CUDA out of memory\" in str(e):\n",
    "                    print(\"Vectorizing text on GPU 1...\")\n",
    "                    torch.cuda.set_device(1)\n",
    "                    torch.cuda.empty_cache()\n",
    "                    df_post = clean_and_vectorize(df, fix_punctuation, sentvec, compDevice, False, chunk_size_setting, batch_size_setting)\n",
    "                    df_post.to_parquet(xVarFilename, engine=\"pyarrow\")\n",
    "                else:\n",
    "                    # Re-raise if it's not an OOM error\n",
    "                    raise\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Read vectorized data\n",
    "XX = []\n",
    "for sentvec in vectors:\n",
    "    xVarFilename = datasetFolder + dataset + \"-X-\" + sentvec + \".parquet\"\n",
    "    df_post = dd.read_parquet(xVarFilename, engine='pyarrow')\n",
    "    XX.append(df_post.compute().to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c39f6-ac70-4b2e-af41-3a2f9e1deb52",
   "metadata": {},
   "source": [
    "## Machine learning (CV using FOR loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d3c559-0bbf-4202-9e7c-804340332860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoflip_score(y_true, y_scores):\n",
    "    # Calculate ROC AUC scores for both variants\n",
    "    auc_score = roc_auc_score(y_true, y_scores)\n",
    "    auc_score_inverted = roc_auc_score(y_true, 1 - y_scores)\n",
    "    # Determine which AUC is higher and return the corresponding scores\n",
    "    if auc_score > auc_score_inverted:\n",
    "        return y_scores\n",
    "    else:\n",
    "        return 1 - y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d00832b-ac28-410e-ac48-f5ecfdad0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture_text --path $cvLogFilename\n",
    "\n",
    "X = range(len(y))\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "cb_task_type = 'GPU' if compDevice == 'cuda' else 'CPU'\n",
    "\n",
    "# Predicted scores to concatenate results across validation folds\n",
    "tst_idx, ground_truth = [], []\n",
    "os_scores = [[] for _ in vectors]  # One list per vectorizer\n",
    "cb_scores = [[] for _ in vectors]  # One list per vectorizer\n",
    "\n",
    "i = 0 # Perform the k-fold cross-validation\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    imbalance_scalar = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    tst_idx.extend(test_index)\n",
    "    ground_truth.extend(y_test.tolist())\n",
    "\n",
    "    print(\"\\nCV fold %d/%d\" % (i + 1, k), flush=True)\n",
    "\n",
    "    os_fold_scores = []\n",
    "    cb_fold_scores = []\n",
    "\n",
    "    # One-class (1c) classification: Histogram-based outlier score (HBOS)\n",
    "    for j, sentvec in enumerate(vectors):\n",
    "        start = pd.Timestamp.now()\n",
    "        X_train, X_test = XX[j][train_index], XX[j][test_index]\n",
    "        if useDimensionalityReduction:\n",
    "            pca = PCA(n_components=num_vars)\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "        clf = HBOS(contamination=0.01)\n",
    "        clf.fit(X_train[y_train==1,:])\n",
    "        score = -1 * clf.decision_function(X_test) / 10000 # make score more aesthetic (calibration)\n",
    "        score = autoflip_score(y_test, score) # HBOS is 1c but ROC/PRC are for 2c so some sanity check\n",
    "        print(\"pyodHBOS %s AUC=%5.3f\" % (vectors[j], roc_auc_score(y_test, score)))\n",
    "        os_scores[j].extend(score.tolist())\n",
    "        print(str(pd.Timestamp.now()-start))\n",
    "        \n",
    "\n",
    "    # Two-class (2c) classification: CatBoost classifier (detection task)\n",
    "    for j, sentvec in enumerate(vectors):\n",
    "        start = pd.Timestamp.now()\n",
    "        X_train, X_test = XX[j][train_index], XX[j][test_index]\n",
    "        if useDimensionalityReduction:\n",
    "            pca = PCA(n_components=num_vars)\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "        model = CatBoostClassifier(iterations=num_tree, learning_rate=0.05, task_type=cb_task_type, allow_writing_files=False,\n",
    "                                   eval_metric=eval_kpi, scale_pos_weight=imbalance_scalar)\n",
    "        X_trn, X_val, y_trn, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=42)\n",
    "        model.fit(X_trn, y_trn, eval_set=(X_val, y_val), use_best_model=True, early_stopping_rounds=10, metric_period=5, verbose=False)\n",
    "        score = model.predict_proba(X_test)[:, 1]\n",
    "        if useCatBoostOutputCallibration:\n",
    "            prob_pos_val = model.predict_proba(X_val)[:, 1]\n",
    "            beta_calibrator = BetaCalibration(parameters=\"abm\")\n",
    "            beta_calibrator.fit(prob_pos_val, y_val)\n",
    "            score = beta_calibrator.predict(score)\n",
    "    \n",
    "        print(\"catBoost %s AUC=%5.3f\" % (vectors[j], roc_auc_score(y_test, score)))\n",
    "        cb_scores[j].extend(score.tolist())\n",
    "        print(str(pd.Timestamp.now()-start))\n",
    "        \n",
    "    i = i + 1\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbdafcf-5302-4828-b0c4-70bd6bcda01b",
   "metadata": {},
   "source": [
    "## Detection task results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5405d2-ebd4-433c-8e96-595d4008ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC\n",
    "roc_results = []\n",
    "fig, ax3 = plt.subplots()\n",
    "colors = sns.color_palette(\"pastel\", len(os_scores))\n",
    "\n",
    "for i, vec_scores in enumerate(os_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(ground_truth, vec_scores, pos_label=1)\n",
    "    roc_auc = roc_auc_score(ground_truth, vec_scores)\n",
    "    roc_results.append({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds, 'auc': roc_auc})\n",
    "    label = f\"1c {vectors[i]} (AUC={roc_auc:.3f})\"\n",
    "    ax3.plot(fpr, tpr, label=label, color=colors[i])\n",
    "\n",
    "for i, vec_scores in enumerate(cb_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(ground_truth, vec_scores, pos_label=1)\n",
    "    roc_auc = roc_auc_score(ground_truth, vec_scores)\n",
    "    roc_results.append({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds, 'auc': roc_auc})\n",
    "    label = f\"2c {vectors[i]} (AUC={roc_auc:.3f})\"\n",
    "    ax3.plot(fpr, tpr, label=label, color=f\"C{i}\")\n",
    "\n",
    "# Plot diagonal reference line\n",
    "ax3.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "\n",
    "ax3.set_xlabel('False Positive Rate')\n",
    "ax3.set_ylabel('True Positive Rate')\n",
    "ax3.set_title(f\"{dataset} (n={len(y)}, target={100*np.sum(y==1)/len(y):.2f}%) → ROC\")\n",
    "ax3.set_xlim([-0.02, 1.02])\n",
    "ax3.set_ylim([-0.02, 1.02])\n",
    "ax3.set_aspect('equal', adjustable='box')\n",
    "ax3.grid(color='darkgrey', linestyle='-', linewidth=0.5)\n",
    "ax3.legend(loc='lower right')\n",
    "plt.show()\n",
    "fig.savefig(rocPlotFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d06b4-b086-41ca-88f5-3a25f7261b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PRC\n",
    "prc_results = []\n",
    "fig, ax3 = plt.subplots()\n",
    "colors = sns.color_palette(\"pastel\", len(os_scores))\n",
    "for i, vec_scores in enumerate(os_scores):\n",
    "    precision, recall, _ = precision_recall_curve(ground_truth, vec_scores, pos_label=1)\n",
    "    ap = average_precision_score(ground_truth, vec_scores, pos_label=1)\n",
    "    prc_results.append(ap)\n",
    "    label = f\"1c {vectors[i]} (AUC={ap:.3f})\"\n",
    "    ax3.plot(recall, precision, label=label, color=colors[i])\n",
    "for i, vec_scores in enumerate(cb_scores):\n",
    "    precision, recall, _ = precision_recall_curve(ground_truth, vec_scores, pos_label=1)\n",
    "    ap = average_precision_score(ground_truth, vec_scores, pos_label=1)\n",
    "    prc_results.append(ap)\n",
    "    label = f\"1c {vectors[i]} (AUC={ap:.3f})\"\n",
    "    ax3.plot(recall, precision, label=label, color=f\"C{i}\")\n",
    "ax3.set_title(f\"{dataset} (n={len(y)}, target={100*np.sum(y==1)/len(y):.2f}%) → PRC\")\n",
    "ax3.set_xlabel(\"Recall\")\n",
    "ax3.set_ylabel(\"Precision\")\n",
    "ax3.set_xlim([-0.02, 1.02])\n",
    "ax3.set_ylim([-0.02, 1.02])\n",
    "ax3.set_aspect('equal', adjustable='box')\n",
    "ax3.grid(color='darkgrey', linestyle='-', linewidth=0.5)\n",
    "ax3.legend(loc='lower left')\n",
    "plt.show()\n",
    "fig.savefig(prcPlotFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa01cf-0cdb-45ef-bbe4-d4962d5c35c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture_text --path $resultsTableFile\n",
    "\n",
    "# Constants\n",
    "kappa_line = \"cohens kappa %4.2f\\n\"\n",
    "title_line = '\\n-----------------------------------------------------\\n'\n",
    "\n",
    "# Ultra-compact score collections\n",
    "models = [('1c', os_scores), ('2c', cb_scores)]\n",
    "\n",
    "# Initialize list to store CSV data\n",
    "csv_data = []\n",
    "\n",
    "# Process all scores\n",
    "result_idx = 0\n",
    "for prefix, scores_list in models:\n",
    "    for i, vec_scores in enumerate(scores_list):\n",
    "        roc, ap = roc_results[result_idx], prc_results[result_idx]\n",
    "        variant_name = vectors[i]\n",
    "\n",
    "        # EER threshold\n",
    "        fpr, tpr, thresholds = roc['fpr'], roc['tpr'], roc['thresholds']\n",
    "        specificity = 1 - fpr\n",
    "        eer_idx = np.argmin(np.abs(tpr - specificity))\n",
    "        optimal_threshold = thresholds[eer_idx]\n",
    "\n",
    "        # Predictions and metrics\n",
    "        predictions = np.array(vec_scores) > optimal_threshold\n",
    "        accuracy = accuracy_score(ground_truth, predictions)\n",
    "        kappa = cohen_kappa_score(ground_truth, predictions)\n",
    "\n",
    "        # Print stylized results (original format)\n",
    "        print(\n",
    "            f'\\n'\n",
    "            f'{prefix} {variant_name} : {dataset} AUC-ROC = {roc[\"auc\"]:.3f}\\n'\n",
    "            f'{prefix} {variant_name} : {dataset} AUC-PRC = {ap:.3f}\\n'\n",
    "            f'Threshold (EER) = {optimal_threshold:.6f}\\n'\n",
    "            f'{title_line}'\n",
    "            f'{classification_report(ground_truth, predictions)}\\n'\n",
    "            f'{kappa_line % kappa}'\n",
    "            f'{title_line}'\n",
    "        )\n",
    "\n",
    "        # Collect data for CSV\n",
    "        csv_data.append({\n",
    "            'Model': f'{prefix} {variant_name}',\n",
    "            'Threshold': float(optimal_threshold),\n",
    "            'Accuracy': accuracy * 100.0,\n",
    "            'Kappa': kappa,\n",
    "            'AUC-ROC': roc[\"auc\"],\n",
    "            'AUC-PRC': ap,\n",
    "        })\n",
    "\n",
    "        result_idx += 1\n",
    "\n",
    "# Save results to CSV with exact column order\n",
    "df = pd.DataFrame(\n",
    "    csv_data,\n",
    "    columns=['Model', 'Threshold', 'Accuracy', 'Kappa', 'AUC-ROC', 'AUC-PRC']\n",
    ")\n",
    "csv_filename = resultsTableSummary\n",
    "df.to_csv(csv_filename, index=False, float_format='%.3f')\n",
    "print(f'\\nResults saved to CSV: {csv_filename}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74985dfb-f89a-4c08-9ca2-9dc83bfb1354",
   "metadata": {},
   "source": [
    "## Model for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8bfeca-70b4-48fa-91a5-2785d219b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Retrain the best 2c CatBoost model on the full dataset and save in -model.pkl format\n",
    "\n",
    "cb_task_type = \"GPU\" if compDevice == \"cuda\" else \"CPU\"\n",
    "\n",
    "# 1. Load summary table and select best 2c model by AUC-ROC\n",
    "summary_df = pd.read_csv(resultsTableSummary)\n",
    "summary_df_2c = summary_df[summary_df[\"Model\"].str.startswith(\"2c \")].copy()\n",
    "if summary_df_2c.empty:\n",
    "    raise ValueError(\"No 2c models found in results table to select best model from.\")\n",
    "best_row = summary_df_2c.loc[summary_df_2c[\"AUC-ROC\"].idxmax()]\n",
    "best_model_name = best_row[\"Model\"]          # e.g. \"2c e5\"\n",
    "best_vector_name = best_model_name.split()[1]\n",
    "best_vector_link = get_vectorizer_link(best_vector_name)\n",
    "best_threshold = float(best_row[\"Threshold\"])  # EER threshold from CSV\n",
    "\n",
    "print(f\"Best model: {best_model_name} with AUC-ROC={best_row['AUC-ROC']:.3f}\")\n",
    "print(f\"Vectorizer link: {best_vector_link}\")\n",
    "\n",
    "# Load full dataset (in vectorized format)\n",
    "X_full = XX[vectors.index(best_vector_name)]\n",
    "y_full = y.to_numpy() if hasattr(y, \"to_numpy\") else np.asarray(y)\n",
    "\n",
    "# PCA transform for dimensionality reduction\n",
    "if useDimensionalityReduction:\n",
    "    pca_full = PCA(n_components=num_vars)\n",
    "    X_full = pca_full.fit_transform(X_full)\n",
    "else:\n",
    "    pca_full = None\n",
    "\n",
    "# Build CatBoost model on the full dataset\n",
    "pos_count = (y_full == 1).sum()\n",
    "neg_count = (y_full == 0).sum()\n",
    "if pos_count == 0:\n",
    "    raise ValueError(\"No positive samples in y_full; cannot compute scale_pos_weight.\")\n",
    "imbalance_scalar_full = neg_count / pos_count\n",
    "X_trn, X_val, y_trn, y_val = train_test_split(X_full, y_full, stratify=y_full, test_size=0.2, random_state=42)\n",
    "final_model = CatBoostClassifier(iterations=num_tree, learning_rate=0.05, task_type=cb_task_type,\n",
    "    allow_writing_files=False, eval_metric=eval_kpi, scale_pos_weight=imbalance_scalar_full)\n",
    "final_model.fit(X_trn, y_trn, eval_set=(X_val, y_val), use_best_model=True,\n",
    "                early_stopping_rounds=10, metric_period=5, verbose=False)\n",
    "\n",
    "# Reproduce calibration using validation scores (if enabled)\n",
    "if useCatBoostOutputCallibration:\n",
    "    prob_pos_val = final_model.predict_proba(X_val)[:, 1]\n",
    "    beta_calibrator = BetaCalibration(parameters=\"abm\")\n",
    "    beta_calibrator.fit(prob_pos_val, y_val)\n",
    "else:\n",
    "    beta_calibrator = None\n",
    "\n",
    "# Package everything for later inference (including EER threshold)\n",
    "model_package = {\n",
    "    \"dataset\": dataset,\n",
    "    \"vectorizer\": best_vector_name,\n",
    "    \"vectorizer_link\": best_vector_link,\n",
    "    \"pca_transform\": pca_full,\n",
    "    \"catboost_model\": final_model,\n",
    "    \"beta_calibrator\": beta_calibrator,\n",
    "    \"threshold\": best_threshold\n",
    "}\n",
    "\n",
    "# Save model to results folder with '-model' suffix\n",
    "model_filename = os.path.join(resultsFolder, f\"{dataset}-model.pkl\")\n",
    "joblib.dump(model_package, model_filename)\n",
    "\n",
    "print(f\"Final best model ({best_model_name}) saved to: {model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
