{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde328bf-a270-4538-a3cd-e316bcbc3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install sentence-transformers einops hf_xet\n",
    "!pip install ninja pyarrow\n",
    "!pip install ftfy emoji\n",
    "!pip install catboost\n",
    "!pip install --no-deps dask-expr\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a459b1c7-ba3c-4f0e-89b1-798b4f8c2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time, os, re, pickle\n",
    "from datetime import datetime\n",
    "import torch, ftfy, emoji, faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import multiprocessing as mp\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d42e23-6e27-4a6f-9340-5713a8892cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "dataset = \"EnSuperset\"\n",
    "datasetAugment = \"EnToxiGen\"\n",
    "datasetFolder = './data/'\n",
    "\n",
    "# CV strategy and results folder with date\n",
    "k_folds = 3\n",
    "start_date = None # Set to '20260104' to reuse results\n",
    "if start_date is None:\n",
    "    start_date = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "resultsFolder = f'./results_{start_date}_{k_folds}foldCV/'\n",
    "os.makedirs(resultsFolder, exist_ok=True)\n",
    "print(f\"Results folder: {resultsFolder}\")\n",
    "\n",
    "compDevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size_setting = 16\n",
    "num_tree = 500\n",
    "stop_rounds = 20\n",
    "lrn_rate = 0.05\n",
    "cb_depth = 8\n",
    "eval_kpi = \"AUC\"\n",
    "num_vars = 64\n",
    "val_frac = 0.2\n",
    "\n",
    "vectors = [\"e5+pca\", \"snow+pca\", \"jina+pca\", \"potion\"]\n",
    "k_neighbors_list = [1, 2, 3]\n",
    "\n",
    "augmentation_fractions = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05]\n",
    "#augmentation_fractions = [0.0, 0.005, 0.010, 0.015, 0.020, 0.025, 0.030, 0.035, 0.040, 0.045, 0.05]\n",
    "replace_with_neighbors = False\n",
    "filter_augmentation_pool = False\n",
    "\n",
    "# Global plot settings\n",
    "PLOT_MIN_AUC = 0.9\n",
    "PLOT_MAX_AUC = 1.0\n",
    "\n",
    "# Ultrafast prototyping\n",
    "FAKE_MODEL_MODE = True\n",
    "\n",
    "# Random augmentation\n",
    "BASELINE_REPEATS = 5 # Runs for random augmentation at max fraction\n",
    "SHOW_BASELINE_RANGE = True # Show min-max range for random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181cb0cf-9b2b-48f4-94ff-a71f49f7904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. DATA PROCESSING & HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "def fix_punctuation(text, toneDown=True):\n",
    "    if hasattr(text, '__len__'):\n",
    "        text = ftfy.fix_text(text)\n",
    "        rules = [(r'https?://\\S+', ''), (r'([,\\.?!])\\1{2,}', r'\\1\\1'),\n",
    "                 (r'([,\\.?!])(?=[^\\s])', r'\\1 '), (r'\\s+([,\\.?!])', r'\\1'),\n",
    "                 (r'\\s{2,}', ' '), (r'(\\d)(?=[^\\s\\d,\\.?!-])', r'\\1 '),\n",
    "                 (r'(?<=[^\\s\\d-])(\\d)', r' \\1')]\n",
    "        if toneDown:\n",
    "            rules.append((r'[?!]', '.'))\n",
    "        text = emoji.demojize(text, delimiters=(\" ::\", \":: \"))\n",
    "        for pattern, replacement in rules:\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        text = text.strip()\n",
    "    else:\n",
    "        text = ''\n",
    "    return text\n",
    "\n",
    "def clean_and_vectorize(df, sentvec=\"e5\", device=\"cuda\"):\n",
    "    if sentvec == \"jina\":\n",
    "        st = SentenceTransformer('jinaai/' + sentvec + '-embeddings-v3', trust_remote_code=True, device=device)\n",
    "    elif sentvec == \"snow\":\n",
    "        st = SentenceTransformer('Snowflake/' + sentvec + 'flake-arctic-embed-l-v2.0', device=device)    \n",
    "    elif sentvec == \"potion\":\n",
    "        st = SentenceTransformer('minishlab/' + sentvec + '-base-2M', device=device)\n",
    "    else:\n",
    "        st = SentenceTransformer('intfloat/multilingual-' + sentvec + '-large-instruct', device=device)\n",
    "    \n",
    "    n_partitions = mp.cpu_count()\n",
    "    ddf = dd.from_pandas(df, npartitions=n_partitions)\n",
    "    texts = ddf.apply(lambda x: fix_punctuation(x.iloc[1]), axis=1, meta=pd.Series(dtype=\"str\")).compute(scheduler=\"processes\").tolist()\n",
    "    embeddings = st.encode(texts, batch_size=batch_size_setting, show_progress_bar=True, normalize_embeddings=True)\n",
    "    \n",
    "    df_embeddings = pd.DataFrame(embeddings)\n",
    "    df_embeddings.columns = [f'X{i+1}' for i in range(df_embeddings.shape[1])]\n",
    "    return df_embeddings\n",
    "\n",
    "def get_error_mask(y_true, probs, optimal_threshold):\n",
    "    \"\"\"\n",
    "    Mark high-confidence false positives and false negatives\n",
    "    given ground-truth labels and predicted probabilities.\n",
    "    \"\"\"\n",
    "    if probs is None:\n",
    "        return None\n",
    "\n",
    "    false_alarms = (y_true == 0) & (probs > optimal_threshold)\n",
    "    false_negatives = (y_true == 1) & (probs < optimal_threshold)\n",
    "    return false_alarms | false_negatives\n",
    "\n",
    "def normalize_query_vectors(query_vectors):\n",
    "    query_vectors = np.ascontiguousarray(query_vectors, dtype=np.float32)\n",
    "    faiss.normalize_L2(query_vectors)\n",
    "    return query_vectors\n",
    "\n",
    "def build_faiss_index(vectors, ids, use_gpu=False):\n",
    "    \"\"\"\n",
    "    Build a cosine-similarity FAISS index with GLOBAL ids.\n",
    "\n",
    "    vectors: np.ndarray [n_samples, dim]\n",
    "    ids:     np.ndarray [n_samples], global indices into X_aug_full / y_augment_full\n",
    "    \"\"\"\n",
    "    vectors = normalize_query_vectors(vectors)\n",
    "    d = vectors.shape[1]\n",
    "\n",
    "    base_index = faiss.IndexFlatIP(d)\n",
    "    index = faiss.IndexIDMap(base_index)\n",
    "\n",
    "    if use_gpu:\n",
    "        try:\n",
    "            res = faiss.StandardGpuResources()\n",
    "            index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "        except Exception as e:\n",
    "            print(f\" GPU failed ({e}), using CPU\")\n",
    "\n",
    "    index.add_with_ids(vectors, ids.astype(\"int64\"))\n",
    "    return index\n",
    "\n",
    "def find_knn_augmentation_indices_faiss(X_train, y_train, train_probs, optimal_threshold,\n",
    "                                        X_aug_pool, y_aug_pool, k, target_aug_size,\n",
    "                                        faiss_indices_by_label, pool_probs=None):\n",
    "    \"\"\"\n",
    "    Smart k-NN augmentation with optional filtering of 'bad' pool points.\n",
    "\n",
    "    pool_probs[i] = p(y=1 | X_aug_pool[i]) from CatBoost.\n",
    "    Pool points that are high-confidence errors w.r.t. optimal_threshold\n",
    "    (same criterion as for train) are skipped as neighbors.\n",
    "    \"\"\"\n",
    "    problematic_mask = get_error_mask(y_train, train_probs, optimal_threshold)\n",
    "    problematic_indices_global = np.where(problematic_mask)[0]\n",
    "    n_problematic = len(problematic_indices_global)\n",
    "\n",
    "    if n_problematic == 0 or target_aug_size == 0:\n",
    "        empty = np.array([], dtype=int)\n",
    "        return empty, empty\n",
    "\n",
    "    X_problematic = X_train[problematic_mask].astype(np.float32, copy=False)\n",
    "    y_problematic = y_train[problematic_mask]\n",
    "    prob_predictions = train_probs[problematic_mask]\n",
    "\n",
    "    priority_scores = np.abs(prob_predictions - optimal_threshold)\n",
    "    sorted_indices = np.argsort(priority_scores)[::-1]\n",
    "\n",
    "    pool_errors = None\n",
    "    if pool_probs is not None:\n",
    "        pool_errors = get_error_mask(y_aug_pool, pool_probs, optimal_threshold)\n",
    "\n",
    "    selected_aug_indices = []\n",
    "    selected_problematic_global = []\n",
    "\n",
    "    # boolean used mask instead of set\n",
    "    used = np.zeros(len(y_aug_pool), dtype=bool)\n",
    "\n",
    "    for local_prob_idx in sorted_indices:\n",
    "        if len(selected_aug_indices) >= target_aug_size:\n",
    "            break\n",
    "\n",
    "        prob_vector = X_problematic[local_prob_idx:local_prob_idx + 1]\n",
    "        prob_label = y_problematic[local_prob_idx]\n",
    "\n",
    "        faiss_index = faiss_indices_by_label[prob_label]\n",
    "        k_search = min(k, faiss_index.ntotal)\n",
    "        if k_search == 0:\n",
    "            continue\n",
    "\n",
    "        _, nn_ids = faiss_index.search(prob_vector, k_search)\n",
    "        candidates = nn_ids[0]\n",
    "\n",
    "        # Build a mask over candidates instead of multiple continues\n",
    "        mask = np.ones_like(candidates, dtype=bool)\n",
    "\n",
    "        if pool_errors is not None:\n",
    "            mask &= ~pool_errors[candidates]\n",
    "\n",
    "        # label consistency (should normally be redundant)\n",
    "        mask &= (y_aug_pool[candidates] == prob_label)\n",
    "\n",
    "        # not already used\n",
    "        mask &= ~used[candidates]\n",
    "\n",
    "        valid_candidates = candidates[mask]\n",
    "        if valid_candidates.size == 0:\n",
    "            continue\n",
    "\n",
    "        # how many we still need\n",
    "        remaining = target_aug_size - len(selected_aug_indices)\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "\n",
    "        take = valid_candidates[:remaining]\n",
    "        selected_aug_indices.extend(take.tolist())\n",
    "        used[take] = True\n",
    "        selected_problematic_global.extend(\n",
    "            [problematic_indices_global[local_prob_idx]] * len(take)\n",
    "        )\n",
    "\n",
    "        if len(selected_aug_indices) >= target_aug_size:\n",
    "            break\n",
    "\n",
    "    return np.array(selected_aug_indices, dtype=int), np.array(selected_problematic_global, dtype=int)\n",
    "\n",
    "def fit_catboost_and_predict(X_train, y_train, X_val, y_val, X_train_full, X_test, X_aug_pool=None,\n",
    "                             cb_device='CPU', num_tree=200, eval_kpi=\"AUC\"):\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    model = CatBoostClassifier(iterations=num_tree, learning_rate=lrn_rate, depth=cb_depth, task_type=cb_device, allow_writing_files=False,\n",
    "                               eval_metric=eval_kpi, loss_function='Logloss', scale_pos_weight=scale_pos_weight, verbose=0)\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), metric_period=5,\n",
    "              early_stopping_rounds=stop_rounds, use_best_model=True, verbose=False)\n",
    "    train_probs = model.predict_proba(X_train_full)[:, 1]\n",
    "    val_probs   = model.predict_proba(X_val)[:, 1]\n",
    "    test_probs  = model.predict_proba(X_test)[:, 1]\n",
    "    aug_pool_probs = None\n",
    "    if X_aug_pool is not None:\n",
    "        aug_pool_probs = model.predict_proba(X_aug_pool)[:, 1]\n",
    "    return train_probs, val_probs, test_probs, aug_pool_probs\n",
    "\n",
    "def get_intermediate_results_filename(vec_name, frac, k, run, fold_i):\n",
    "    if frac == 0.0 and k == 0 and run == 0:\n",
    "        return f\"{vec_name}_fold{fold_i+1}.pkl\"\n",
    "    frac_str = f\"{int(frac*1000):03d}\"\n",
    "    if run == 0:\n",
    "        filename = f\"{vec_name}_frac{frac_str}_k{k}_fold{fold_i+1}.pkl\"\n",
    "    else:\n",
    "        filename = f\"{vec_name}_frac{frac_str}_r{run}_fold{fold_i+1}.pkl\"\n",
    "    return filename\n",
    "\n",
    "def save_intermediate_results(vec_name, frac, k, run, fold_i, y_train, y_val, y_test, \n",
    "                             train_probs, val_probs, test_probs, results_folder):\n",
    "    filename = get_intermediate_results_filename(vec_name, frac, k, run, fold_i)\n",
    "    filepath = os.path.join(results_folder, filename)\n",
    "    \n",
    "    results_dict = {\n",
    "        'y_train': y_train, 'y_val': y_val, 'y_test': y_test,\n",
    "        'train_probs': train_probs, 'val_probs': val_probs, 'test_probs': test_probs,\n",
    "        'vectorizer': vec_name, 'fraction': frac, 'k_neighbors': k, 'run': run,'fold': fold_i\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(results_dict, f)\n",
    "    print(f\"      Saved: {filename}\")\n",
    "    return filepath\n",
    "\n",
    "def load_intermediate_results(vec_name, frac, k, run, fold_i, results_folder):\n",
    "    filename = get_intermediate_results_filename(vec_name, frac, k, run, fold_i)\n",
    "    filepath = os.path.join(results_folder, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            results_dict = pickle.load(f)\n",
    "        print(f\"      Loaded: {filename}\")\n",
    "        return results_dict\n",
    "    return None\n",
    "\n",
    "def generate_fake_predictions(y_true, base_auc=0.7, noise_level=0.1, random_state=None):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    y_true = np.asarray(y_true)\n",
    "    scores = y_true + rng.normal(0, noise_level, len(y_true))\n",
    "    ranks = scores.argsort().argsort()\n",
    "    probs = ranks / (len(ranks) - 1)\n",
    "    return base_auc * probs + (1 - base_auc) * rng.random(len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151c01c-0fb1-46e0-b98c-950112f880a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ==========================================\n",
    "# 3. LOAD DATASETS\n",
    "# ==========================================\n",
    "print(\"Loading datasets...\")\n",
    "if dataset == 'EnSuperset':\n",
    "    df = pd.read_csv(datasetFolder + dataset + '.csv', engine='python', header=0)\n",
    "    if 'labels' in df.columns:\n",
    "        target_col = 'labels'\n",
    "        text_col = [c for c in df.columns if c != target_col][0]\n",
    "        df = df[[target_col, text_col]]\n",
    "    elif isinstance(df.iloc[0, 0], str): \n",
    "        df = df[df.columns[::-1]]\n",
    "else:\n",
    "    df = pd.read_csv(datasetFolder + dataset + '.csv', engine='python', header=0)\n",
    "    if 'prompt_label' in df.columns:\n",
    "        target_col = 'prompt_label'\n",
    "        text_col = [c for c in df.columns if c != target_col][0]\n",
    "        df = df[[target_col, text_col]]\n",
    "    elif isinstance(df_augment.iloc[0, 0], str):\n",
    "        df = df[df.columns[::-1]]\n",
    "df.columns = range(df.shape[1])\n",
    "y_main = df[0].to_numpy()\n",
    "\n",
    "if datasetAugment == 'EnToxiGen':\n",
    "    df_augment = pd.read_csv(datasetFolder + datasetAugment + '.csv', engine='python', header=0)\n",
    "    if 'prompt_label' in df_augment.columns:\n",
    "        target_col = 'prompt_label'\n",
    "        text_col = [c for c in df_augment.columns if c != target_col][0]\n",
    "        df_augment = df_augment[[target_col, text_col]]\n",
    "    elif isinstance(df_augment.iloc[0, 0], str):\n",
    "        df_augment = df_augment[df_augment.columns[::-1]]\n",
    "else:\n",
    "    df_augment = pd.read_csv(datasetFolder + datasetAugment + '.csv', engine='python', header=0)\n",
    "    if 'labels' in df_augment.columns:\n",
    "        target_col = 'labels'\n",
    "        text_col = [c for c in df_augment.columns if c != target_col][0]\n",
    "        df_augment = df_augment[[target_col, text_col]]\n",
    "    elif isinstance(df_augment.iloc[0, 0], str): \n",
    "        df_augment = df_augment[df_augment.columns[::-1]]\n",
    "df_augment.columns = range(df_augment.shape[1])\n",
    "y_augment_full = df_augment[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f7909c-f3db-49fb-ac1e-79e887def123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. PRE-CALCULATE VECTORS AND BUILD FAISS INDICES\n",
    "# ==========================================\n",
    "vectors_cache = {v: {} for v in vectors}\n",
    "faiss_indices_cache = {v: {} for v in vectors}\n",
    "vector_dims = {}\n",
    "use_faiss_gpu = False\n",
    "\n",
    "for sentvec in vectors:\n",
    "    print(f\"\\nProcessing: {sentvec}\")\n",
    "    parts = sentvec.split(\"+\", 1)\n",
    "    sentvecModel = parts[0]\n",
    "    transformPCA = len(parts) > 1 and parts[1] == \"pca\"\n",
    "\n",
    "    # Main set vectors\n",
    "    f_path = f\"{datasetFolder}{dataset}-X-{sentvecModel}.parquet\"\n",
    "    if not os.path.exists(f_path):\n",
    "        df_vec = clean_and_vectorize(df, sentvec=sentvecModel, device=compDevice)\n",
    "        df_vec.to_parquet(f_path, engine=\"pyarrow\")\n",
    "    X_main = pd.read_parquet(f_path).to_numpy()\n",
    "\n",
    "    # Augmentation pool vectors (ToxiGen)\n",
    "    f_path_aug = f\"{datasetFolder}{datasetAugment}-X-{sentvecModel}.parquet\"\n",
    "    if not os.path.exists(f_path_aug):\n",
    "        df_vec_aug = clean_and_vectorize(df_augment, sentvec=sentvecModel, device=compDevice)\n",
    "        df_vec_aug.to_parquet(f_path_aug, engine=\"pyarrow\")\n",
    "    X_aug = pd.read_parquet(f_path_aug).to_numpy()\n",
    "\n",
    "    # Optional joint PCA\n",
    "    if transformPCA:\n",
    "        pca = PCA(n_components=num_vars)\n",
    "        merged = np.vstack([X_main, X_aug])\n",
    "        pca.fit(merged)\n",
    "        merged_transformed = pca.transform(merged)\n",
    "        n_main = len(X_main)\n",
    "        X_main = merged_transformed[:n_main]\n",
    "        X_aug = merged_transformed[n_main:]\n",
    "        del merged, merged_transformed\n",
    "        vector_dims[sentvec] = num_vars\n",
    "    else:\n",
    "        vector_dims[sentvec] = X_main.shape[1]\n",
    "\n",
    "    vectors_cache[sentvec][\"main\"] = X_main\n",
    "    vectors_cache[sentvec][\"aug\"] = X_aug\n",
    "\n",
    "    # Build label-specific FAISS indices with GLOBAL ids\n",
    "    global_ids = np.arange(len(y_augment_full), dtype=np.int64)\n",
    "    faiss_indices_cache[sentvec] = {}\n",
    "\n",
    "    for label in [0, 1]:\n",
    "        label_mask = (y_augment_full == label)\n",
    "        X_aug_label = X_aug[label_mask].copy()\n",
    "        ids_label = global_ids[label_mask]\n",
    "\n",
    "        if len(ids_label) == 0:\n",
    "            # No samples of this label in the pool\n",
    "            faiss_indices_cache[sentvec][label] = faiss.IndexIDMap(\n",
    "                faiss.IndexFlatIP(vector_dims[sentvec])\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        index = build_faiss_index(X_aug_label, ids_label, use_gpu=use_faiss_gpu)\n",
    "        faiss_indices_cache[sentvec][label] = index\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Vector preparation and FAISS indexing complete!\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b28f5e-767b-4f84-8d6e-ff99b7692669",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ==========================================\n",
    "# 5. SMART k-NN AUGMENTATION EXPERIMENT\n",
    "# ==========================================\n",
    "max_frac = max(augmentation_fractions)\n",
    "final_results = {\n",
    "    v: {k: {f: None for f in augmentation_fractions} for k in [0] + k_neighbors_list}\n",
    "    for v in vectors\n",
    "}\n",
    "baseline_results = {v: [] for v in vectors}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "cb_device = 'GPU' if compDevice == 'cuda' else 'CPU'\n",
    "\n",
    "# Optional: precompute probabilities on the augmentation pool per vectorizer.\n",
    "# If unused, leave as None and filtering will be skipped.\n",
    "aug_pool_probs_per_vec = {v: None for v in vectors}\n",
    "\n",
    "for vec_name in vectors:\n",
    "    print(f\"\\n>>> {vec_name}\")\n",
    "\n",
    "    X_main = vectors_cache[vec_name]['main']\n",
    "    X_aug_full = vectors_cache[vec_name]['aug']\n",
    "\n",
    "    # ----- Baseline (no augmentation) -----\n",
    "    pooled_y_true, pooled_y_probs = [], []\n",
    "\n",
    "    for fold_i, (train_idx, test_idx) in enumerate(skf.split(X_main, y_main)):\n",
    "        if not FAKE_MODEL_MODE:\n",
    "            existing = load_intermediate_results(vec_name, 0.0, 0, 0, fold_i, resultsFolder)\n",
    "            if existing:\n",
    "                pooled_y_true.extend(existing['y_test'])\n",
    "                pooled_y_probs.extend(existing['test_probs'])\n",
    "                continue\n",
    "\n",
    "        X_train_fold, X_test_fold = X_main[train_idx], X_main[test_idx]\n",
    "        y_train_fold, y_test_fold = y_main[train_idx], y_main[test_idx]\n",
    "\n",
    "        X_in_train, X_in_val, y_in_train, y_in_val = train_test_split(\n",
    "            X_train_fold, y_train_fold, test_size=val_frac,\n",
    "            stratify=y_train_fold, random_state=42\n",
    "        )\n",
    "\n",
    "        if FAKE_MODEL_MODE:\n",
    "            train_probs = generate_fake_predictions(y_train_fold, base_auc=0.69, noise_level=0.1)\n",
    "            val_probs   = generate_fake_predictions(y_in_val,   base_auc=0.69, noise_level=0.1)\n",
    "            test_probs  = generate_fake_predictions(y_test_fold, base_auc=0.69, noise_level=0.1)\n",
    "        else:\n",
    "            train_probs, val_probs, test_probs, _ = fit_catboost_and_predict(\n",
    "                X_in_train, y_in_train, X_in_val, y_in_val,\n",
    "                X_train_fold, X_test_fold, X_aug_pool=None,\n",
    "                cb_device=cb_device, num_tree=num_tree, eval_kpi=eval_kpi\n",
    "            )\n",
    "\n",
    "            save_intermediate_results(vec_name, 0.0, 0, 0, fold_i, y_train_fold, y_in_val, y_test_fold,\n",
    "                                      train_probs, val_probs, test_probs, resultsFolder)\n",
    "\n",
    "        pooled_y_true.extend(y_test_fold)\n",
    "        pooled_y_probs.extend(test_probs)\n",
    "\n",
    "    baseline_auc = roc_auc_score(pooled_y_true, pooled_y_probs)\n",
    "    final_results[vec_name][0][0.0] = baseline_auc\n",
    "    for k in k_neighbors_list:\n",
    "        final_results[vec_name][k][0.0] = baseline_auc\n",
    "    print(f\" Base AUC: {baseline_auc:.4f}\")\n",
    "\n",
    "    # ----- k-NN augmentation -----\n",
    "    for frac in augmentation_fractions:\n",
    "        if frac == 0.0:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n frac={frac:.3f}\")\n",
    "        target_aug_size = int(len(y_main) * frac)\n",
    "\n",
    "        for k in k_neighbors_list:\n",
    "            print(f\" k={k}\")\n",
    "            pooled_y_true, pooled_y_probs = [], []\n",
    "\n",
    "            for fold_i, (train_idx, test_idx) in enumerate(skf.split(X_main, y_main)):\n",
    "                if not FAKE_MODEL_MODE:\n",
    "                    existing = load_intermediate_results(vec_name, frac, k, 0, fold_i, resultsFolder)\n",
    "                    if existing:\n",
    "                        pooled_y_true.extend(existing['y_test'])\n",
    "                        pooled_y_probs.extend(existing['test_probs'])\n",
    "                        continue\n",
    "\n",
    "                X_train_fold, X_test_fold = X_main[train_idx], X_main[test_idx]\n",
    "                y_train_fold, y_test_fold = y_main[train_idx], y_main[test_idx]\n",
    "\n",
    "                all_indices = np.arange(len(y_train_fold))\n",
    "                train_sub_idx, val_sub_idx = train_test_split(\n",
    "                    all_indices, test_size=val_frac,\n",
    "                    stratify=y_train_fold, random_state=42\n",
    "                )\n",
    "\n",
    "                X_in_train = X_train_fold[train_sub_idx]\n",
    "                y_in_train = y_train_fold[train_sub_idx]\n",
    "                X_in_val   = X_train_fold[val_sub_idx]\n",
    "                y_in_val   = y_train_fold[val_sub_idx]\n",
    "\n",
    "                if FAKE_MODEL_MODE:\n",
    "                    base_auc_plus = 0.7 + frac / k\n",
    "                    train_probs = generate_fake_predictions(y_train_fold, base_auc=base_auc_plus, noise_level=0.1)\n",
    "                    val_probs   = generate_fake_predictions(y_in_val,   base_auc=base_auc_plus, noise_level=0.1)\n",
    "                    test_probs  = generate_fake_predictions(y_test_fold, base_auc=base_auc_plus, noise_level=0.1)\n",
    "                else:\n",
    "                    if filter_augmentation_pool:\n",
    "                        X_aug_pool = X_aug_full\n",
    "                    else:\n",
    "                        X_aug_pool = None\n",
    "                    \n",
    "                    # Initial model to get train_probs and threshold\n",
    "                    init_train_probs, _, _, aug_pool_probs = fit_catboost_and_predict(\n",
    "                        X_in_train, y_in_train, X_in_val, y_in_val,\n",
    "                        X_train_fold, X_test_fold, X_aug_pool=X_aug_pool,\n",
    "                        cb_device=cb_device, num_tree=num_tree, eval_kpi=eval_kpi\n",
    "                    )\n",
    "\n",
    "                    fpr, tpr, thresholds = roc_curve(y_train_fold, init_train_probs)\n",
    "                    optimal_threshold = thresholds[np.argmin(np.abs(tpr + fpr - 1))]\n",
    "\n",
    "                    aug_indices, problematic_global = find_knn_augmentation_indices_faiss(\n",
    "                        X_train_fold, y_train_fold, init_train_probs, optimal_threshold,\n",
    "                        X_aug_full, y_augment_full, k, target_aug_size,\n",
    "                        faiss_indices_cache[vec_name], aug_pool_probs)\n",
    "\n",
    "                    if len(aug_indices) > 0:\n",
    "                        X_aug = X_aug_full[aug_indices]\n",
    "                        y_aug = y_augment_full[aug_indices]\n",
    "\n",
    "                        if replace_with_neighbors:\n",
    "                            in_train_mask = np.zeros(len(y_train_fold), dtype=bool)\n",
    "                            in_train_mask[train_sub_idx] = True\n",
    "                            problematic_in_train = problematic_global[in_train_mask[problematic_global]]\n",
    "\n",
    "                            pos_in_train = -np.ones(len(y_train_fold), dtype=int)\n",
    "                            pos_in_train[train_sub_idx] = np.arange(len(train_sub_idx))\n",
    "                            problematic_positions = pos_in_train[problematic_in_train]\n",
    "\n",
    "                            keep_mask = np.ones(len(X_in_train), dtype=bool)\n",
    "                            keep_mask[problematic_positions] = False\n",
    "\n",
    "                            X_in_train_kept = X_in_train[keep_mask]\n",
    "                            y_in_train_kept = y_in_train[keep_mask]\n",
    "\n",
    "                            X_in_train_aug = np.concatenate([X_in_train_kept, X_aug], axis=0)\n",
    "                            y_in_train_aug = np.concatenate([y_in_train_kept, y_aug], axis=0)\n",
    "                        else:\n",
    "                            X_in_train_aug = np.concatenate([X_in_train, X_aug], axis=0)\n",
    "                            y_in_train_aug = np.concatenate([y_in_train, y_aug], axis=0)\n",
    "\n",
    "                        shuffle_idx = np.random.permutation(len(X_in_train_aug))\n",
    "                        X_in_train_aug = X_in_train_aug[shuffle_idx]\n",
    "                        y_in_train_aug = y_in_train_aug[shuffle_idx]\n",
    "                    else:\n",
    "                        X_in_train_aug, y_in_train_aug = X_in_train, y_in_train\n",
    "\n",
    "                    train_probs, val_probs, test_probs, _ = fit_catboost_and_predict(\n",
    "                        X_in_train_aug, y_in_train_aug, X_in_val, y_in_val,\n",
    "                        X_train_fold, X_test_fold, X_aug_pool=None,\n",
    "                        cb_device=cb_device, num_tree=num_tree, eval_kpi=eval_kpi\n",
    "                    )\n",
    "\n",
    "                    save_intermediate_results(vec_name, frac, k, 0, fold_i, y_train_fold, y_in_val, y_test_fold,\n",
    "                                              train_probs, val_probs, test_probs, resultsFolder)\n",
    "\n",
    "                pooled_y_true.extend(y_test_fold)\n",
    "                pooled_y_probs.extend(test_probs)\n",
    "\n",
    "            pooled_auc = roc_auc_score(pooled_y_true, pooled_y_probs)\n",
    "            final_results[vec_name][k][frac] = pooled_auc\n",
    "            print(f\" AUC: {pooled_auc:.4f} (Δ={pooled_auc - baseline_auc:+.4f})\")\n",
    "\n",
    "    # ----- Random augmentation baseline at max fraction -----\n",
    "    print(\"\\n\")\n",
    "    for repeat in range(BASELINE_REPEATS):\n",
    "        aug_size = int(len(y_main) * max_frac)\n",
    "        np.random.seed(42 + repeat)\n",
    "        aug_indices = np.random.choice(len(y_augment_full), size=aug_size, replace=False)\n",
    "        X_aug = X_aug_full[aug_indices]\n",
    "        y_aug = y_augment_full[aug_indices]\n",
    "\n",
    "        pooled_y_true, pooled_y_probs = [], []\n",
    "\n",
    "        for fold_i, (train_idx, test_idx) in enumerate(skf.split(X_main, y_main)):\n",
    "            if not FAKE_MODEL_MODE:\n",
    "                existing = load_intermediate_results(vec_name, max_frac, 0, repeat + 1, fold_i, resultsFolder)\n",
    "                if existing:\n",
    "                    pooled_y_true.extend(existing['y_test'])\n",
    "                    pooled_y_probs.extend(existing['test_probs'])\n",
    "                    continue\n",
    "\n",
    "            X_train_fold, X_test_fold = X_main[train_idx], X_main[test_idx]\n",
    "            y_train_fold, y_test_fold = y_main[train_idx], y_main[test_idx]\n",
    "\n",
    "            X_in_train, X_in_val, y_in_train, y_in_val = train_test_split(\n",
    "                X_train_fold, y_train_fold, test_size=val_frac,\n",
    "                stratify=y_train_fold, random_state=42\n",
    "            )\n",
    "\n",
    "            X_in_train_aug = np.concatenate([X_in_train, X_aug], axis=0)\n",
    "            y_in_train_aug = np.concatenate([y_in_train, y_aug], axis=0)\n",
    "            shuf = np.random.permutation(len(X_in_train_aug))\n",
    "            X_in_train_aug = X_in_train_aug[shuf]\n",
    "            y_in_train_aug = y_in_train_aug[shuf]\n",
    "\n",
    "            if FAKE_MODEL_MODE:\n",
    "                base_auc_rand = 0.7 - max_frac / 2\n",
    "                train_probs = generate_fake_predictions(y_train_fold, base_auc=base_auc_rand, noise_level=0.1)\n",
    "                val_probs   = generate_fake_predictions(y_in_val,   base_auc=base_auc_rand, noise_level=0.1)\n",
    "                test_probs  = generate_fake_predictions(y_test_fold, base_auc=base_auc_rand, noise_level=0.1)\n",
    "            else:\n",
    "                train_probs, val_probs, test_probs, _ = fit_catboost_and_predict(\n",
    "                    X_in_train_aug, y_in_train_aug, X_in_val, y_in_val,\n",
    "                    X_train_fold, X_test_fold, X_aug_pool=None,\n",
    "                    cb_device=cb_device, num_tree=num_tree, eval_kpi=eval_kpi)\n",
    "\n",
    "                save_intermediate_results(vec_name, max_frac, 0, repeat + 1, fold_i, y_train_fold, y_in_val, y_test_fold,\n",
    "                                          train_probs, val_probs, test_probs, resultsFolder)\n",
    "\n",
    "            pooled_y_true.extend(y_test_fold)\n",
    "            pooled_y_probs.extend(test_probs)\n",
    "\n",
    "        repeat_auc = roc_auc_score(pooled_y_true, pooled_y_probs)\n",
    "        baseline_results[vec_name].append(repeat_auc)\n",
    "        print(f\" Rand AUC: {repeat_auc:.4f} ({repeat + 1}/{BASELINE_REPEATS})\")\n",
    "\n",
    "    mean_auc = np.mean(baseline_results[vec_name])\n",
    "    min_auc, max_auc = np.min(baseline_results[vec_name]), np.max(baseline_results[vec_name])\n",
    "    print(f\" Mean AUC: {mean_auc:.4f} (Δ={mean_auc - baseline_auc:+.4f})\")\n",
    "    print(f\" Range AUC: [{min_auc:.4f}, {max_auc:.4f}]\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a370e-95ca-4ecc-b2f5-2d7484cf2802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6. PLOTTING k-NN RESULTS WITH BASELINE AT MAX FRACTION\n",
    "# ==========================================\n",
    "print(\"\\nGenerating plots...\")\n",
    "\n",
    "n_plots = len(vectors)\n",
    "n_cols = 2\n",
    "n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 6 * n_rows), squeeze=False)\n",
    "\n",
    "for idx, vec_name in enumerate(vectors):\n",
    "    row = idx // n_cols\n",
    "    col = idx % n_cols\n",
    "    ax = axes[row][col]\n",
    "    \n",
    "    baseline_auc = final_results[vec_name][0][0.0]\n",
    "    aug_fracs = [f for f in augmentation_fractions if f > 0.0]\n",
    "    \n",
    "    # Original baseline (k=0, frac=0) - Black dashed line\n",
    "    ax.axhline(y=baseline_auc, color='black', linestyle='--', linewidth=2, alpha=0.7,\n",
    "               label=f\"NoAug AUC={baseline_auc:.3f} (baseline)\")\n",
    "    \n",
    "    # Random augmentation baseline statistics\n",
    "    baseline_mean = np.mean(baseline_results[vec_name])\n",
    "    baseline_min = np.min(baseline_results[vec_name])\n",
    "    baseline_max = np.max(baseline_results[vec_name])\n",
    "\n",
    "    # Random baseline: Light gray band for min-max range\n",
    "    if SHOW_BASELINE_RANGE:\n",
    "        ax.axhspan(baseline_min, baseline_max, color='lightgray', alpha=0.3, \n",
    "                   label=f'rand min AUC={baseline_min:.3f} @ {max_frac:.3f}')    \n",
    "    \n",
    "    # Random baseline: Dark gray dashed line at mean\n",
    "    ax.axhline(y=baseline_mean, color='darkgray', linestyle='--', linewidth=2, alpha=0.7,\n",
    "               label=f'rand mean AUC={baseline_mean:.3f} @ {max_frac:.3f}')\n",
    "\n",
    "    # Random baseline: Light gray band for min-max range\n",
    "    if SHOW_BASELINE_RANGE:\n",
    "        ax.axhspan(baseline_min, baseline_max, color='lightgray', alpha=0.3, \n",
    "                   label=f'rand max AUC={baseline_max:.3f} @ {max_frac:.3f}')    \n",
    "    \n",
    "    # k-NN results\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(k_neighbors_list)))\n",
    "    \n",
    "    for k_idx, k in enumerate(k_neighbors_list):\n",
    "        aucs = [final_results[vec_name][k][f] for f in aug_fracs]\n",
    "        max_auc = max(aucs)\n",
    "        max_idx = aucs.index(max_auc)\n",
    "        max_frac_knn = aug_fracs[max_idx]\n",
    "        \n",
    "        ax.plot(aug_fracs, aucs, marker='o', color=colors[k_idx], linewidth=2, markersize=6,\n",
    "                label=f\"k={k} max AUC={max_auc:.3f} @ {max_frac_knn:.3f}\")\n",
    "    \n",
    "    ax.set_xlabel(\"Fraction of Augmentation Data\", fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel(\"AUC ROC\", fontsize=11, fontweight='bold')\n",
    "    dims = vector_dims[vec_name] # Add dimensionality to title\n",
    "    ax.set_title(f\"{vec_name} ({dims} dims)\", fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    ax.grid(True, axis='y', linestyle=':', linewidth=0.7, alpha=0.35)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.set_ylim(PLOT_MIN_AUC, PLOT_MAX_AUC)\n",
    "\n",
    "# Hide unused subplots\n",
    "if n_plots % n_cols != 0:\n",
    "    for idx in range(n_plots, n_rows * n_cols):\n",
    "        row = idx // n_cols\n",
    "        col = idx % n_cols\n",
    "        axes[row][col].set_visible(False)\n",
    "\n",
    "plot_path = f\"{resultsFolder}{k_folds}foldCV_results_kNN_vs_Rand_Augment.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Plot saved: {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7da2f-a9c6-46c6-aa69-293ac12f5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 7. SAVE AUC ROC RESULTS\n",
    "# ==========================================\n",
    "results_list = []\n",
    "for vec in vectors:\n",
    "    baseline = final_results[vec][0][0.0]\n",
    "    dims = vector_dims[vec]\n",
    "    results_list.append({\"Vectorizer\": vec, \"Dims\": dims, \"Method\": \"NoAug\", \"Frac\": 0.0, \"k\": 0, \"Run\": 0, \"AUC\": baseline, \"Delta\": 0.0})\n",
    "    for k in k_neighbors_list:\n",
    "        for frac in augmentation_fractions:\n",
    "            auc = final_results[vec][k][frac]\n",
    "            if auc is not None:\n",
    "                results_list.append({\"Vectorizer\": vec, \"Dims\": dims, \"Method\": \"kNN\", \"Frac\": frac, \"k\": k, \"Run\": 0, \"AUC\": auc, \"Delta\": auc - baseline})\n",
    "    for run, auc in enumerate(baseline_results[vec]):\n",
    "        results_list.append({\"Vectorizer\": vec, \"Dims\": dims, \"Method\": \"Rand\", \"Frac\": max_frac, \"k\": 0, \"Run\": run+1, \"AUC\": auc, \"Delta\": auc - baseline})\n",
    "df_res = pd.DataFrame(results_list)\n",
    "csv_path = f\"{resultsFolder}{k_folds}foldCV_results_kNN_vs_Rand_Augment.csv\"\n",
    "df_res.to_csv(csv_path, index=False)\n",
    "print(f\"CSV saved: {csv_path}\")\n",
    "print(\"\\nEXPERIMENT COMPLETE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
